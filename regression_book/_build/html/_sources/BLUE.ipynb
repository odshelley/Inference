{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Linear Unbiased Estimators (BLUE)\n",
    "\n",
    "Note that in the least squares derivation of the model parameters, there was no statistical inference as such, we merely fitted a line to the data according to some criterion. We will now show that the $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ as derived by OLS are optimal in a statistical sense. \n",
    "\n",
    "In particular, we will derive unbiased linear estimators for $\\beta_0$ and $\\beta_1$ that have the smallest possible variance. Such an estimator will be called a *best* estimator. An estimator is linear if it is of the form\n",
    "\n",
    "````{math}\n",
    ":label: eq_estimator\n",
    "    \\sum_{i=1}^n d_i Y_i\n",
    "````\n",
    "where $\\{d_i\\}_{i=1}^n$ are fixed constants. If {eq}`eq_estimator` is an unbiased estimator of $\\beta_1$, then \n",
    "````{math}\n",
    "    \\beta_1 = \\mathbb{E}\\left[\\sum_{i=1}^n d_i Y_i\\right] =  \\beta_0  \\sum_{i=1}^n d_i  + \\beta_1 \\sum_{i=1}^n x_i d_i,\n",
    "````\n",
    "whence \n",
    "````{math}\n",
    ":label: eq_betaConditions\n",
    "     \\sum_{i=1}^n d_i = 0 \\quad \\text{ and } \\quad  \\sum_{i=1}^n x_i d_i = 1.\n",
    "````\n",
    "Note that \n",
    "````{math}\n",
    ":label: eq_varianceCondition\n",
    "     \\text{Var}\\left[ \\sum_{i=1}^n d_i Y_i \\right] = \\sigma^2 \\sum_{i=1}^n d_i^2.\n",
    "````\n",
    "Thus, to find the BLUE estimator for $\\beta_1$ we must find $\\{d_i\\}_{i=1}^n$ that satisfy {eq}`eq_betaConditions` and minimize {eq}`eq_varianceCondition`. Similarly, to find the BLUE estimator for $\\beta_0$ we must find $\\{d_i\\}_{i=1}^n$ that minimize {eq}`eq_varianceCondition` and satisfy\n",
    "\n",
    "````{math}\n",
    ":label: eq_alphaConditions\n",
    "     \\sum_{i=1}^n d_i = 1 \\quad \\text{ and } \\quad  \\sum_{i=1}^n x_i d_i = 0.\n",
    "````\n",
    "\n",
    "One could use Lagrange multipliers to find the coefficients, but we will rely on the following lemma.  For a proof see [Lemma 11.2.7]{cite}`casella_statistical_2002`.\n",
    "\n",
    "````{prf:lemma}\n",
    ":label: lemma_casella\n",
    "\n",
    "Let $\\left(v_1, \\ldots, v_k\\right)$ be constants and let $\\left(c_1, \\ldots, c_k\\right)$ be positive constants. Then, for $\\mathcal{A}=\\left\\{\\mathbf{a}=\\left(a_1, \\ldots, a_k\\right): \\sum a_i=0\\right\\}$,\n",
    "\n",
    "$$\n",
    "\\max _{\\mathbf{a} \\in \\mathcal{A}}\\left\\{\\frac{\\left(\\sum_{i=1}^k a_i v_i\\right)^2}{\\sum_{i=1}^k a_i^2 / c_i}\\right\\}=\\sum_{i=1}^k c_i\\left(v_i-\\bar{v}_c\\right)^2\n",
    "$$\n",
    "\n",
    "where $\\bar{v}_c=\\frac{\\sum c_i v_i}{ \\sum c_i }$. The maximum is attained at any $\\mathbf{a}$ of the form $a_i=K c_i\\left(v_i-\\right.$ $\\left.\\bar{v}_c\\right)$, where $K$ is a nonzero constant.\n",
    "````\n",
    "We may now state the following\n",
    "\n",
    "````{prf:proposition}\n",
    ":label: prop_blue \n",
    "\n",
    "The BLUE estimators for $\\beta_1$ and $\\beta_0$ are given by \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta}_1 = \\frac{S_{xY}}{S_{xx}} \\quad \\text{ and } \\quad \\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{x},\n",
    "\\end{aligned}\n",
    "$$\n",
    "respectively.\n",
    "````\n",
    "\n",
    "````{prf:proof}\n",
    "\n",
    "We begin by calculating $\\hat{\\beta}_1$.  Fix a non-zero constant $K$ and let $d_i := K(x_i - \\bar{x})$. Then according to {prf:ref}`lemma_casella`, $(d_1,\\dots,d_n)$ maximises \n",
    "\n",
    "```{math}\n",
    ":label: eq_toMax\n",
    "    \\frac{\\left(\\sum_{i=1}^n d_i x_i\\right)^2}{\\sum_{i=1}^n d_i^2}\n",
    "```\n",
    "\n",
    "among all $(d_1,\\dots,d_n)$ that satisfy $\\sum_{i=1}^n d_i=0$. In particular, if we let $K = \\frac{1}{S_{xx}}$, then $\\sum_{i=1}^n d_i x_i = 1$, and so $(d_1,\\dots,d_n)$ maximizes {eq}`eq_toMax` among all $(d_1,\\dots,d_n)$ that satisfy {eq}`eq_betaConditions`. Moreover, in this case {eq}`eq_toMax` becomes\n",
    "\n",
    "```{math}\n",
    "    \\frac{1}{\\sum_{i=1}^n d_i^2},\n",
    "```\n",
    "whence $(d_1,\\dots,d_n)$ satisfies {eq}`eq_betaConditions` and {eq}`eq_varianceCondition`. We may conclude that $\\sum_{i=1}^n d_i Y_i$ is the BLUE estimator for $\\beta_1$ where\n",
    "\n",
    "```{math}\n",
    "    \\sum_{i=1}^n d_i Y_i = \\sum_{i=1}^n \\frac{(x_i - \\bar{x})Y_i}{S_{xx}} = \\frac{S_xY}{S_{xx}} = \\hat{\\beta}_1.\n",
    "```\n",
    "\n",
    "$\\quad$ We now turn to calculating $\\hat{\\beta}_o$. Fix $K$ to be a non-zero constant and define \n",
    "\n",
    "```{math}\n",
    "    \\tilde{d}_i := d_ix_i := K x_i^2 \\left( \\frac{1}{x_i} -  \\frac{\\sum_{i=1}^n x_i^2\\left(\\frac{1}{x_i}\\right)}{\\sum_{i=1}^n x_i^2} \\right)\n",
    "```\n",
    "Then by {prf:ref}`lemma_casella`,  $(\\tilde{d}_1,\\dots,\\tilde{d}_n)$ maximizes \n",
    "\n",
    "```{math}\n",
    ":label: eq_toMax2\n",
    "    \\frac{\\left(\\sum_{i=1}^n \\frac{\\tilde{d}_i}{x_i}\\right)^2}{\\sum_{i=1}^n \\frac{\\tilde{d}_i^2}{x_i^2}} \n",
    "```\n",
    "among all $(\\tilde{d}_1,\\dots,\\tilde{d}_n)$ that satisfy $\\sum_{i=1}^n \\tilde{d}_i=0$. In particular, we note that by choosing \n",
    "\n",
    "```{math}\n",
    "    K = \\frac{\\sum_{i=1}^n x_i^2}{n S_{xx}} = \\frac{\\sum_{i=1}^n x_i^2}{n \\left(\\sum_{i=1}^2 x_i^2 - n\\bar{x}^2\\right)} = \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\n",
    "```\n",
    "it follows that \n",
    "```{math}\n",
    "\\begin{aligned}\n",
    "    d_i &= K \\left(1 -  \\frac{n \\bar{x}x_i }{\\sum_{i=1}^n x_i^2} \\right) = \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} - \\frac{ \\bar{x}x_i }{S_{xx}} = \\frac{1}{n} - \\frac{ \\bar{x}(x_i-\\bar{x}) }{S_{xx}}.\n",
    "\\end{aligned}\n",
    "``` \n",
    "Thus, with this choice of $K$ we have $\\sum_{i=1}^n d_i = 1$ and {eq}`eq_toMax2` becomes\n",
    "```{math}\n",
    "    \\frac{\\left(\\sum_{i=1}^n d_i\\right)^2}{\\sum_{i=1}^n {d}_i^2} = \\frac{1}{\\sum_{i=1}^n {d}_i^2},\n",
    "```\n",
    "whence $(d_1,\\dots,d_n)$ satisfies {eq}`eq_alphaConditions` and {eq}`eq_varianceCondition`. We may conclude that $\\sum_{i=1}^n d_i Y_i$ is the BLUE estimator for $\\beta_0$ where\n",
    "\n",
    "```{math}\n",
    "    \\sum_{i=1}^n d_i Y_i = \\sum_{i=1}^n \\left(\\frac{1}{n} - \\frac{ \\bar{x}(x_i-\\bar{x}) }{S_{xx}} \\right)Y_i = \\bar{Y} - \\hat{\\beta}_1\\bar{x}.\n",
    "```\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
