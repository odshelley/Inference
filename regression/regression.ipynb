{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Overview\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "Suppose that we have data $\\mathcal{D}:=\\{(x_i,y_i):i \\in \\{1,\\dots,n\\}\\}$ where $\\{(x_i,y_i)\\}_{i=1}^n$ are observed values of random variables $\\{Y_i\\}_{i=1}^n$ and $\\{X_i\\}_{i=1}^n$ respectively, satisfying the relationship\n",
    "\\begin{equation}\n",
    "    Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i.\n",
    "\\end{equation}\n",
    "Here, $\\{\\varepsilon_i\\}_{i=1}^n$ are uncorrelated mean zero random variables with common variance $\\sigma^2$, which we call *random errors*. \n",
    "***\n",
    "**Definition:** For random variables $Y$ and $X$ we define the *population regression function* (or merely the regression) of $Y$ with respect to $X$ as the function $f:\\mathbb{R} \\to \\mathbb{R}$ such that \n",
    "\\begin{equation*}\n",
    "    f(x) = \\mathbb{E}\\left[ Y | X = x \\right].\n",
    "\\end{equation*}\n",
    "***\n",
    "In general, the word *regression* is used in statistics to signify a relationship between variables. From equation $(1)$, we can see that the regression function of any $Y_i$ with respect to $X_i$ is of the form \n",
    "\\begin{equation*}\n",
    "    \\mathbb{E}\\left[ Y_i | X_i = x_i \\right]  = \\beta_0 + \\beta_1 x_i,\n",
    "\\end{equation*}\n",
    "which is a linear function of $x_i$. Thus, equation (1) defines a *linear regression*. One main purpose of regression is to predict $Y_i$ from of instances of $X_i$, and so it is common to refer to $Y_i$ as the response variable and $X_i$ as the predictor variable. The quantities $\\beta_0$ and $\\beta_1$ are called the *intercept* and *slope*, respectively, and are assumed to be fixed and unknown. Together they are known as the model *coefficients* or *parameters*. It is these unknown parameters that we wish to estimate, so that we can describe the relationship between the $Y_i$ and $X_i$.\n",
    "\n",
    "***\n",
    "**Definition:** Let $\\mathcal{D}:=\\{(x_i,y_i):i \\in \\{1,\\dots,n\\}\\}$.\n",
    "-  We define the *sample means* as \n",
    "\\begin{equation*}\n",
    "    \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i \\quad \\text{ and } \\quad  \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i.\n",
    "\\end{equation*}\n",
    "- We define the *sums of squares* as \n",
    "\\begin{equation*}\n",
    "    S_{xx} = \\sum_{i=1}^n (x_i-\\bar{x})^2 \\quad \\text{ and } \\quad  S_{yy} = \\sum_{i=1}^n (y_i-\\bar{y})^2.\n",
    "\\end{equation*}\n",
    "- We definethe *sums of cross-products* as \n",
    "\\begin{equation*}\n",
    "    S_{xy} = \\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y}).\n",
    "\\end{equation*}\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares (OLS)\n",
    "\n",
    "Our first estimation of $\\beta_0$ and $\\beta_1$ will involve drawing a straight line through the data $\\mathcal{D}$ that 'comes as close as possible' to all the points. In particular, let $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ be a straight line that predicts the value of $Y_i$ based on the observed value $x_i$ of $X_i$. Then $e_i = y_i - \\hat{y}_i$ represents the $i$-th *residual*, and we aim to find estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that minimise the *residual sum of squares* \n",
    "\\begin{equation}\n",
    "    \\text{RSS}(a,b) := \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n ( y_i - (a + b x_i) )^2,\n",
    "\\end{equation}\n",
    "i.e.\n",
    "\\begin{equation*}\n",
    "    (\\hat{\\beta}_0,\\hat{\\beta}_1) =  \\text{argmin}_{(a,b)} \\sum_{i=1}^n ( y_i - (a + b x_i) )^2.\n",
    "\\end{equation*}\n",
    "If we fix $b$, then the value of $a$ that minimises $(2)$ is \n",
    "\\begin{equation*}\n",
    "    a = \\frac{1}{n}\\sum_{i=1}^n (y_i - b x_i) = \\bar{y} - b \\bar{x}.\n",
    "\\end{equation*}\n",
    "Substituting this value of $a$ into $(1)$ gives us \n",
    "\\begin{equation*}\n",
    "    \\frac{1}{n}\\sum_{i=1}^n (y_i - (  \\bar{y} - b \\bar{x} + b x_i ))^2 = S_{yy} - 2b S_{xy} + b^2 S_{xx}.\n",
    "\\end{equation*}\n",
    "Thus, \n",
    "\\begin{equation*}\n",
    "    \\frac{\\text{d}}{\\text{d}b}  \\text{RSS}(\\bar{y} - b \\bar{x},b) = 0 \\Leftrightarrow b = \\frac{S_{xy}}{S_{xx}},\n",
    "\\end{equation*}\n",
    "whence $\\frac{S_{xy}}{S_{xx}}$ is a global minimum as $\\frac{\\text{d}^2}{\\text{d}b^2}  \\text{RSS}(\\bar{y} - b \\bar{x},b) > 0$. We may conclude that \n",
    "\\begin{equation*}\n",
    "    \\hat{\\beta}_0 = \\bar{y} - \\frac{S_{xy}}{S_{xx}} \\bar{x}   \\quad \\text{and} \\quad\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Linear Unbiased Estimators (BLUE)\n",
    "\n",
    "Note that in the least squares estimation of the model parameters, there was no statistical inference as such, we merely fitted a line to the data according to some criterion. We will now show that the $(\\hat{\\beta}_0,\\hat{\\beta}_1)$ as derived by OLS are optimal in a statistical sense. \n",
    "\n",
    "In particular, we will derive unbiased linear estimators for $\\beta_0$ and $\\beta_1$ that have the smallest possible variance. Such an estimator will be called a *best* estimator. An estimator is linear if it is of the form\n",
    "\\begin{equation}\n",
    "    \\sum_{i=1}^n d_i Y_i\n",
    "\\end{equation}\n",
    "where $\\{d_i\\}_{i=1}^n$ are fixed constants. If (1) is an unbiased estimator of $\\beta_1$, then \n",
    "\\begin{equation*}\n",
    "    \\beta_1 = \\mathbb{E}\\left[\\sum_{i=1}^n d_i Y_i\\right] =  \\beta_0  \\sum_{i=1}^n d_i  + \\beta_1 \\sum_{i=1}^n x_i d_i,\n",
    "\\end{equation*}\n",
    "whence \n",
    "\\begin{equation}\n",
    "     \\sum_{i=1}^n d_i = 0 \\quad \\text{ and } \\quad  \\sum_{i=1}^n x_i d_i = 1.\n",
    "\\end{equation}\n",
    "Note that \n",
    "\\begin{equation}\n",
    "     \\text{Var}\\left[ \\sum_{i=1}^n d_i Y_i \\right] = \\sigma^2 \\sum_{i=1}^n d_i^2.\n",
    "\\end{equation}\n",
    "Thus, to find the BLUE estimator for $\\beta_1$ we must find $\\{d_i\\}_{i=1}^n$ that satisfy (2) and minimize (3). One could use Lagrange multipliers to find the coefficients, but we will rely on the following lemma.\n",
    "***\n",
    "**Lemma:** Let $\\left(v_1, \\ldots, v_k\\right)$ be constants and let $\\left(c_1, \\ldots, c_k\\right)$ be positive constants. Then, for $\\mathcal{A}=\\left\\{\\mathbf{a}=\\left(a_1, \\ldots, a_k\\right): \\sum a_i=0\\right\\}$,\n",
    "$$\n",
    "\\max _{\\mathbf{a} \\in \\mathcal{A}}\\left\\{\\frac{\\left(\\sum_{i=1}^k a_i v_i\\right)^2}{\\sum_{i=1}^k a_i^2 / c_i}\\right\\}=\\sum_{i=1}^k c_i\\left(v_i-\\bar{v}_c\\right)^2\n",
    "$$\n",
    "where $\\bar{v}_c=\\frac{\\sum c_i v_i}{ \\sum c_i }$. The maximum is attained at any $\\mathbf{a}$ of the form $a_i=K c_i\\left(v_i-\\right.$ $\\left.\\bar{v}_c\\right)$, where $K$ is a nonzero constant.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statisticalLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
